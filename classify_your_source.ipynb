{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify your own X-ray source\n",
    "\n",
    "---\n",
    "\n",
    "To classify your own X-ray source (or multiple sources), ensure that you have the following attributes with valid values:\n",
    "\n",
    "`'hard_hm', 'hard_hs', 'hard_ms', 'powlaw_gamma', 'bb_kt', 'var_prob_*', 'var_mean_*', 'var_sigma_*', 'var_max_b', 'var_min_b'`\n",
    "\n",
    "Here, the asterisk (*) represents 'b', 'h', or 's'.\n",
    "\n",
    "**Note:** Use data from individual detections, not aggregated master properties. The more detections you have, the more robust and insightful your classification will be.\n",
    "\n",
    "For detailed descriptions of each feature and how to obtain them, consult the [Chandra Source Catalog documentation](https://cxc.harvard.edu/csc/).\n",
    "\n",
    "Proceed by running the cells below. They contain all the necessary code and modified functions to classify a new X-ray source based on the original dataset and methodology presented in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from umlcaxs_lib import votable_to_pandas, mahal_classifier_all\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Feature definitions\n",
    "features = ['hard_hm', 'hard_hs', 'hard_ms', 'powlaw_gamma', 'bb_kt', 'var_prob_b', 'var_ratio_b', 'var_prob_h', 'var_ratio_h', 'var_prob_s', 'var_ratio_s', 'var_newq_b']\n",
    "features_lognorm = ['bb_kt', 'var_ratio_h', 'var_ratio_b', 'var_ratio_s', 'var_newq_b']\n",
    "features_norm = ['powlaw_gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(original_data, target_data, features, features_norm, features_lognorm):\n",
    "    transformed_target = target_data.copy()\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature in original_data.columns and feature in transformed_target:\n",
    "            \n",
    "            if feature in features_lognorm:\n",
    "                min_val = np.min(original_data[feature][original_data[feature] != 0]) / 10\n",
    "                transformed_data = np.log(original_data[feature] + min_val)\n",
    "            elif feature in features_norm:\n",
    "                transformed_data = original_data[feature]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            scaler = min_max_scaler.fit(transformed_data.values.reshape(-1, 1))\n",
    "            \n",
    "            target_feature_data = np.array([transformed_target[feature]])\n",
    "            \n",
    "            if feature in features_lognorm:\n",
    "                target_feature_data = np.log(target_feature_data + min_val)\n",
    "            \n",
    "            scaled_target_feature_data = scaler.transform(target_feature_data.reshape(-1, 1))\n",
    "            transformed_target[feature] = scaled_target_feature_data.flatten()[0]\n",
    "    \n",
    "    return transformed_target\n",
    "\n",
    "\n",
    "def load_original_data(file_path, features, features_lognorm, features_norm):\n",
    "    data = votable_to_pandas(file_path)\n",
    "    \n",
    "    # Define new measurement columns\n",
    "    data['var_ratio_b'] = data['var_sigma_b'] / data['var_mean_b']\n",
    "    data['var_ratio_h'] = data['var_sigma_h'] / data['var_mean_h']\n",
    "    data['var_ratio_s'] = data['var_sigma_s'] / data['var_mean_s']\n",
    "    data['var_newq_b'] = ((data['var_max_b'] + data['var_min_b']) / 2) / data['var_mean_b']\n",
    "    \n",
    "    # Drop data with missing feature values\n",
    "    data_out = data.dropna(subset=features)\n",
    "    \n",
    "    # Normalize or log-normalize features\n",
    "    return data_out\n",
    "\n",
    "def load_your_xray(file_path, original_data, features, features_lognorm, features_norm, original):\n",
    "    data = votable_to_pandas(file_path)\n",
    "    \n",
    "    # Calculate medians of the original dataset for each feature\n",
    "    feature_medians = original[features].median()\n",
    "    \n",
    "    # Define new measurement columns\n",
    "    data['var_ratio_b'] = data['var_sigma_b'] / data['var_mean_b']\n",
    "    data['var_ratio_h'] = data['var_sigma_h'] / data['var_mean_h']\n",
    "    data['var_ratio_s'] = data['var_sigma_s'] / data['var_mean_s']\n",
    "    data['var_newq_b'] = ((data['var_max_b'] + data['var_min_b']) / 2) / data['var_mean_b']\n",
    "    \n",
    "    # Fill missing values with medians from the original dataset\n",
    "    for feature in features:\n",
    "        data[feature].fillna(feature_medians[feature], inplace=True)\n",
    "    \n",
    "    X_df = data[features]\n",
    "    \n",
    "    # Normalize or log-normalize features\n",
    "    return transform_target(original_data, X_df, features, features_norm, features_lognorm).to_numpy(), data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this example, we will classify a detection for the source '2CXO J004307.5-092302', inspired by a query from my colleague Andrea Sacchi. \n",
    "\n",
    "Ensure that the detection data for your source(s) is available in VOTable format.\n",
    "\n",
    "Modify line 2 in the input argument, replacing `\"data/your_xray_example.vot\"` with the path to your own data file.\n",
    "\n",
    "If your data is correctly formatted, running the subsequent cells should be good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data and give your source a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "cscresults = load_original_data(\"data/cscresults.vot\", features, features_lognorm, features_norm)\n",
    "X_your_xray, data_out_your_xray = load_your_xray(\"data/your_xray_example.vot\", cscresults, features, features_lognorm, features_norm, cscresults)\n",
    "\n",
    "# Load the GMM\n",
    "with open(\"gmm_model.pkl\", \"rb\") as file:\n",
    "    loaded_gmm = pickle.load(file)\n",
    "\n",
    "\n",
    "data_out_your_xray['main_type'] = 'NaN'\n",
    "data_out_your_xray['cluster'] = loaded_gmm.predict(X_your_xray)\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Classify your source(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubunsam/miniconda3/envs/umlcaxs_old/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3269: DtypeWarning: Columns (56,63,64,65,66,67,68,73,74) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Cluster 0***\n",
      "***Cluster 1***\n",
      "***Cluster 2***\n",
      "***Cluster 3***\n",
      "***Cluster 4***\n",
      "***Cluster 5***\n",
      "Classification done.\n"
     ]
    }
   ],
   "source": [
    "ltypes = ['QSO', 'AGN', 'Seyfert_1', 'Seyfert_2', 'HMXB', 'LMXB', 'XB', 'YSO', 'TTau*', 'Orion_V*']\n",
    "uks = ['Star', 'X', 'Radio', 'IR', 'Blue', 'UV', 'gamma', 'PartofG', '**']\n",
    "\n",
    "# Load data\n",
    "df_csc_simbad = pd.read_csv('out_data/cluster_csc_simbad.csv', index_col=0)\n",
    "df_csc_simbad.fillna({'main_type': 'NaN'}, inplace=True)\n",
    "\n",
    "# Preprocess data\n",
    "df_csc_out = df_csc_simbad.dropna(subset=features)\n",
    "\n",
    "df_csc_out_with_target = pd.concat([df_csc_simbad, data_out_your_xray], ignore_index=True)\n",
    "df_csc_lognorm = transform_target(df_csc_out, df_csc_out_with_target, features, features_norm, features_lognorm)\n",
    "\n",
    "# Classification\n",
    "classified_df = mahal_classifier_all(df_csc_lognorm, df_csc_out_with_target, features, ltypes, uks=uks)\n",
    "print(\"Classification done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Review your classification results.\n",
    "\n",
    "In this example, we have classified a single detection for one source. The same approach can be extended to multiple detections and multiple sources. Here, we will focus on examining the classification of just one source. If you wish to explore the classification of multiple sources, you can modify the `name_source` variable accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_source = '2CXO J004307.5-092302'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>obsid</th>\n",
       "      <th>cluster</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>main_type</th>\n",
       "      <th>QSO</th>\n",
       "      <th>AGN</th>\n",
       "      <th>Seyfert_1</th>\n",
       "      <th>Seyfert_2</th>\n",
       "      <th>...</th>\n",
       "      <th>hard_ms</th>\n",
       "      <th>powlaw_gamma</th>\n",
       "      <th>bb_kt</th>\n",
       "      <th>var_prob_b</th>\n",
       "      <th>var_ratio_b</th>\n",
       "      <th>var_prob_h</th>\n",
       "      <th>var_ratio_h</th>\n",
       "      <th>var_prob_s</th>\n",
       "      <th>var_ratio_s</th>\n",
       "      <th>var_newq_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>2CXO J004307.5-092302</td>\n",
       "      <td>4884</td>\n",
       "      <td>5</td>\n",
       "      <td>10.781298</td>\n",
       "      <td>-9.383954</td>\n",
       "      <td>XB</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.263051</td>\n",
       "      <td>0.00796</td>\n",
       "      <td>0.173319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136165</td>\n",
       "      <td>2.07377</td>\n",
       "      <td>0.563729</td>\n",
       "      <td>0.501655</td>\n",
       "      <td>0.18304</td>\n",
       "      <td>0.508149</td>\n",
       "      <td>0.137342</td>\n",
       "      <td>0.562971</td>\n",
       "      <td>0.129812</td>\n",
       "      <td>1.369809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  obsid  cluster         ra       dec main_type  \\\n",
       "1344  2CXO J004307.5-092302   4884        5  10.781298 -9.383954        XB   \n",
       "\n",
       "           QSO       AGN  Seyfert_1  Seyfert_2  ...   hard_ms  powlaw_gamma  \\\n",
       "1344  0.002228  0.263051    0.00796   0.173319  ... -0.136165       2.07377   \n",
       "\n",
       "         bb_kt  var_prob_b  var_ratio_b  var_prob_h  var_ratio_h  var_prob_s  \\\n",
       "1344  0.563729    0.501655      0.18304    0.508149     0.137342    0.562971   \n",
       "\n",
       "      var_ratio_s  var_newq_b  \n",
       "1344     0.129812    1.369809  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_df.query(f\"name == '{name_source}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(AGN)\n",
      " [0.44655752]\n"
     ]
    }
   ],
   "source": [
    "prob_agn = classified_df.query(f\"name == '{name_source}'\").QSO + classified_df.query(f\"name == '{name_source}'\").AGN + classified_df.query(f\"name == '{name_source}'\").Seyfert_1 + classified_df.query(f\"name == '{name_source}'\").Seyfert_2\n",
    "print(\"P(AGN)\\n\", prob_agn.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(XB)\n",
      " [0.55211735]\n"
     ]
    }
   ],
   "source": [
    "prob_xb = classified_df.query(f\"name == '{name_source}'\").XB + classified_df.query(f\"name == '{name_source}'\").LMXB + classified_df.query(f\"name == '{name_source}'\").HMXB\n",
    "print(\"P(XB)\\n\", prob_xb.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(YSO)\n",
      " [0.00132512]\n"
     ]
    }
   ],
   "source": [
    "prob_yso = classified_df.query(f\"name == '{name_source}'\").YSO + classified_df.query(f\"name == '{name_source}'\")['TTau*'] + classified_df.query(f\"name == '{name_source}'\")['Orion_V*']\n",
    "print(\"P(YSO)\\n\", prob_yso.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Save your classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your classification\n",
    "classified_df.query(f\"name == '{name_source}'\").to_csv('your_source_classified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. In the case of multiple detections, create a master hard and soft classification.\n",
    "\n",
    "In this specific example, both classifications will be identical because we are working with only one detection. However, if you have multiple detections for your source, soft and hard classifications can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def mean_std_round(x):\n",
    "    return f\"{round(np.mean(x), 3)}±{round(np.std(x), 3)}\"\n",
    "\n",
    "def most_common(x): \n",
    "    return Counter(x).most_common(1)[0][0]\n",
    "\n",
    "# Soft voting\n",
    "summ_table = classified_df.groupby('name')[ltypes].agg(mean_std_round)\n",
    "summ_table['detection_count'] = classified_df.groupby(['name']).size()\n",
    "ra_dec_df = classified_df.groupby('name')[['ra', 'dec']].first()\n",
    "features_df = classified_df.groupby('name')[features].first()\n",
    "summ_table = summ_table.join([ra_dec_df, features_df])\n",
    "summ_table_prov = classified_df.groupby('name')[ltypes].agg(['mean', 'std'])\n",
    "class_mean_names = [list(tup) for tup in itertools.product(ltypes, ['mean'], repeat=1)]\n",
    "names_comp = summ_table_prov[class_mean_names].idxmax(axis=1).to_list()\n",
    "master_names = [name[0] for name in names_comp]\n",
    "summ_table['soft_master_class'] = master_names\n",
    "summ_table.sort_values('detection_count', ascending=False, inplace=True)\n",
    "\n",
    "# Hard voting\n",
    "summ_table_hard = classified_df.groupby('name').size().to_frame('detection_count')\n",
    "summ_table_hard['hard_master_class'] = classified_df.groupby('name')['main_type'].agg(most_common)\n",
    "ra_dec_df = classified_df.groupby('name')[['ra', 'dec']].first()\n",
    "summ_table_hard = summ_table_hard.join(ra_dec_df)\n",
    "summ_table_hard.sort_values('detection_count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard classification:\n",
      "name\n",
      "2CXO J004307.5-092302    XB\n",
      "Name: hard_master_class, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Hard classification:\")\n",
    "print(summ_table_hard.query(\"name == '2CXO J004307.5-092302'\")['hard_master_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft classification:\n",
      "name\n",
      "2CXO J004307.5-092302    XB\n",
      "Name: soft_master_class, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Soft classification:\")\n",
    "print(summ_table.query(\"name == '2CXO J004307.5-092302'\")['soft_master_class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umlcaxs_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
